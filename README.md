ğŸŒ WEB CRAWLER

A multi-threaded Java Web Crawler with a Spring Boot backend and React frontend.
This project enables configurable crawling parameters, respects domain restrictions, and displays real-time crawl progress and results in a modern web interface.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸš€ Features

â€¢ Concurrent crawling with customizable thread count
â€¢ Domain-restricted link extraction to stay within the target website
â€¢ Configurable parameters: start URL, maximum pages, and crawl depth
â€¢ Real-time progress updates displayed dynamically on the UI
â€¢ Robust HTML parsing with Jsoup
â€¢ Spring Boot REST API backend returning JSON crawl data
â€¢ Interactive React frontend for starting crawls and visualizing results

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ§± Tech Stack

Backend:
Java 17 
Spring Boot 3
Jsoup (HTML parsing)
Maven (build & dependency management)

Frontend:
React (functional components + hooks)
Node.js & npm

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ› ï¸ Getting Started

âœ… Prerequisites

Make sure you have the following installed:

JDK 17

Maven 3.x

Node.js (with npm)

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“¦ Installation

Clone the repository

Build the backend
mvn clean install


Setup the frontend
cd client
npm install


â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â–¶ï¸ Running the Application

ğŸ§© Backend (Spring Boot)

mvn spring-boot:run


The API will be available at:
http://localhost:8080

ğŸ’» Frontend (React)

cd client
npm start


Open your browser at:
http://localhost:3000

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ§­ Usage

Enter a Start URL.

Configure:

Maximum pages

Thread count

Maximum crawl depth

Click â€œStart Crawlâ€ to begin.

View real-time crawl progress and list of crawled URLs.

Crawling stops automatically when limits are reached.
